> # Discriminant analysis
> require(MASS)
> require(dplyr)
> require(ggplot2)
> require(ellipse)
> 
> # Data
> dados<-read.csv("G:/Meu Drive/Projeto/Resultados/Model_13.csv",header=TRUE)
> dados$Outcome<-as.factor(dados$Outcome)
> #dados[,-c(ncol(dados))] <- scale(dados[,-c(ncol(dados))])
> str(dados)
'data.frame':   268 obs. of  11 variables:
 $ dPg    : num  12.8 12.4 7.9 13.3 13.9 14.9 13.2 12.4 13.3 13.9 ...
 $ dHg    : num  18.7 12.9 12.4 11.9 13.5 14.1 13.4 12.9 11.9 13.5 ...
 $ dDg    : num  16.6 17.7 16.5 17.2 17.2 17.1 18.9 17.7 17.2 17.2 ...
 $ dTg    : num  28.1 25.2 22.1 24.8 25.9 26.7 26.7 25.2 24.8 25.9 ...
 $ dPs    : num  3.44 1.5 3.45 1.5 1.5 1.5 1.5 4.1 4.1 4.1 ...
 $ dHs    : num  4.11 4.7 3.69 4.7 4.7 4.7 4.7 2.5 2.5 2.5 ...
 $ dDs    : num  16.2 16 16.2 16 16 16 16 16 16 16 ...
 $ dTs    : num  17.1 16.7 17 16.7 16.7 16.7 16.7 16.7 16.7 16.7 ...
 $ Dist   : num  17.3 13.7 9.79 13.9 15.3 16.4 14.9 13.4 13.2 14.8 ...
 $ FH1    : num  46.4 25.9 9.75 23.6 30.7 36.3 36 27.4 24.9 32.4 ...
 $ Outcome: Factor w/ 3 levels "G","I","S": 3 1 2 1 1 1 1 1 1 1 ...
> 
> # Data partition
> set.seed(555)
> proportion <- 0.995
> k <- nrow(dados)
> outs <- NULL
> 
> for (j in 1:k)
+ {
+ index <- sample(1:nrow(dados), round(proportion*nrow(dados)))
+ train <- dados[index, ]
+ test <- dados[-index, ]
+ 
+ 
+ lda.train <- lda(formula=Outcome~.,data=train,na.action="na.omit")
+ lda.train.p <- predict(lda.train,train)
+ 
+ # Confusion matrix and accuracy - testing data
+ p2 <- predict(lda.train, test)$class
+ tab1 <- table(Predicted = p2, Actual = test$Outcome)
+ outs[j] <- sum(diag(tab1))/sum(tab1)
+ }
> mean(outs)
[1] 0.8320896
> lda.train <- lda(formula=Outcome~.,data=dados,na.action="na.omit")
> lda.train
Call:
lda(Outcome ~ ., data = dados, na.action = "na.omit")

Prior probabilities of groups:
         G          I          S 
0.86194030 0.03731343 0.10074627 

Group means:
       dPg       dHg      dDg      dTg      dPs      dHs      dDs      dTs
G 6.438797  9.310866 16.86450 20.84805 3.883939 3.395628 16.08658 16.95671
I 8.725000 12.291000 17.21000 23.71000 4.338000 4.842000 16.17000 17.59000
S 5.501481  9.639259 16.74074 20.77037 3.840000 4.115556 16.19259 17.22222
      Dist       FH1
G 7.741039  9.533136
I 9.648000 18.532200
S 6.548519  9.412741

Coefficients of linear discriminants:
             LD1        LD2
dPg  -0.12403281  0.2766223
dHg   0.30642185  0.3564541
dDg  -0.18846499 -0.3214409
dTg   0.51364123 -0.7230707
dPs  -0.70714250 -0.8390796
dHs  -0.39939157 -1.4265614
dDs  -0.25662703  4.3972335
dTs   1.25329865  1.8521704
Dist -0.71026290 -0.4130458
FH1   0.05235853  0.1129128

Proportion of trace:
   LD1    LD2 
0.6731 0.3269 
> lda.data <- cbind(dados, predict(lda.train)$x)
> 
> dat_ell <- data.frame()
> for(g in levels(lda.data$Outcome)){
+ dat_ell <- rbind(dat_ell, cbind(as.data.frame(with(lda.data[lda.data$Outcome==g,], ellipse(cor(LD1, LD2), 
+                                          scale=c(sd(LD1),sd(LD2)), 
+                                          centre=c(mean(LD1),mean(LD2))))),Outcome=g))
+ }
> 
> ggplot(lda.data, aes(x=LD1, y=LD2, col=Outcome) ) + 
+ geom_point( size = 3, aes(color = Outcome))+theme_bw()+
+ geom_path(data=dat_ell,aes(x=x,y=y,color=Outcome),size=1,linetype=1)
> 
> 
> 
> 
